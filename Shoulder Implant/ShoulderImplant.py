# -*- coding: utf-8 -*-
"""x-ray.ipynb

Automatically generated by Colaboratory.
"""

import numpy as np
import tensorflow as tf
from tensorflow import keras
import pandas as pd
import matplotlib.pylab as plt
import cv2
import numpy as np
from PIL import Image
from tensorflow.keras.preprocessing import image
import os
from skimage.transform import resize
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers.experimental.preprocessing import Rescaling
from tensorflow.keras import layers
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import precision_recall_fscore_support 
from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions
from sklearn.utils import class_weight

# Path Of Images
filePath = '/content/drive/MyDrive/Datawithoutanything/data/'
#Number of images in file
path, dirs, files = next(os.walk(filePath))
number_images = len(files)

#Convert image to numpy
images = np.ndarray(shape = (number_images, 250,250,3), dtype = np.float32)
labels = np.ndarray(shape = (number_images)) 
image_names = []

i = 0;

#One Hot Encode
for file in os.listdir(filePath):
  if file.split('.')[0] == 'Zimmer':
    labels[i] = 0
    #1,0,0,0
  elif file.split('.')[0] == 'Tornier':
    labels[i] = 1
    #0,1,0,0
  elif file.split('.')[0] == 'Depuy':
    labels[i] = 2
    #0,0,1,0
  elif file.split('.')[0] == 'Cofield':
    labels[i] = 3
    #0,0,0,1

  #read in the image
  pic = plt.imread(filePath + file,format='jpg')
  pic = resize(pic,(250,250,3))
  images[i] = pic
  image_names.append(file)
  i += 1

#Save Image
filePath = '/content/drive/MyDrive/Datawithoutanything/' 
np.save(filePath + 'image_names',np.asarray(image_names))
np.save(filePath + 'images_unstandardized', images)
np.save(filePath + 'labels',labels)

filePath = '/content/drive/MyDrive/FinalFinalData/data/'
dataset = keras.preprocessing.image_dataset_from_directory(filePath,image_size = (250,250), batch_size = 64 )



pretrained = tf.keras.applications.VGG16(
    include_top=False, weights='imagenet')

def build_VGG(pretrained):

  inputs = keras.Input(shape = (250,250,3))

  x = layers.Conv2D(64,(3,3),padding='same', activation = 'relu')(inputs)
  x = layers.Conv2D(64,(3,3),padding='same', activation = 'relu')(x)
  x = layers.MaxPooling2D(pool_size=(2,2), strides = (2,2))(x)

  x = layers.Conv2D(128, (3,3), padding = 'same', activation ='relu')(x)
  x = layers.Conv2D(128, (3,3), padding = 'same', activation ='relu')(x)
  x = layers.MaxPooling2D(pool_size=(2,2), strides = (2,2))(x)

  x = layers.Conv2D(256, (3,3), padding = 'same', activation ='relu')(x)
  x = layers.Conv2D(256, (3,3), padding = 'same', activation ='relu')(x)
  x = layers.Conv2D(256, (3,3), padding = 'same', activation ='relu')(x)
  x = layers.MaxPooling2D(pool_size=(2,2), strides = (2,2))(x)

  x = layers.Conv2D(512, (3,3), padding = 'same', activation ='relu')(x)
  x = layers.Conv2D(512, (3,3), padding = 'same', activation ='relu')(x)
  x = layers.Conv2D(512, (3,3), padding = 'same', activation ='relu')(x)
  x = layers.MaxPooling2D(pool_size=(2,2), strides = (2,2))(x)

  x = layers.Conv2D(512, (3,3), padding = 'same', activation ='relu')(x)
  x = layers.Conv2D(512, (3,3), padding = 'same', activation ='relu')(x)
  x = layers.Conv2D(512, (3,3), padding = 'same', activation ='relu')(x)
  x = layers.MaxPooling2D(pool_size=(2,2), strides = (2,2))(x)

  outputs = layers.Flatten()(x)
  model = keras.Model(inputs, outputs, name = 'VGG16')

  model.compile(optimizer = keras.optimizers.Adam(learning_rate=0.0000001),
                       loss = 'categorical_crossentropy',
                metrics=[keras.metrics.CategoricalAccuracy()
                         ])
  #transfer learning and set weight 
  model.set_weights(pretrained.get_weights())
  assert len(model.weights) == len(pretrained.weights)
  for a, b in zip(model.weights, pretrained.weights):
    np.testing.assert_allclose(a.numpy(), b.numpy())

  x = layers.Dense(512, activation = 'relu')(model.layers[-1].output)
  x =  layers.Dense(4,activation = 'softmax')(x)

  model = keras.Model(inputs = inputs, outputs =x, name = 'VGG16')

  model.compile(optimizer = keras.optimizers.Adam(learning_rate=0.0000001),
                       loss = 'categorical_crossentropy',
                metrics=[keras.metrics.CategoricalAccuracy()
                         ])

  return model

filePath = '/content/drive/MyDrive/Datawithoutanything/'
labels = np.load(filePath + 'labels.npy')
images_raw = np.load(filePath + 'images_unstandardized.npy')

#Normalization
images_raw = images_raw/255

x_train, x_test, y_train, y_test = train_test_split(images_raw, 
                                                     labels.astype(int), 
                                                    test_size=0.2,random_state=42,
                                                    stratify=labels.astype(int))
y_test  = keras.utils.to_categorical(y_test ,4)

kf = StratifiedKFold(n_splits = 2)
validation_errors = []
best_loss = 0

# With only the train data, split into validation/train
for train_index,valid_index in kf.split(x_train,y_train):
  x_train_split, x_valid_split = x_train[train_index], x_train[valid_index]
  y_train_split, y_valid_split = y_train[train_index], y_train[valid_index]

  y_train_split= keras.utils.to_categorical(y_train_split, 4)
  y_valid_split = keras.utils.to_categorical(y_valid_split,4)

  model = build_VGG(pretrained)

  model.fit(x_train_split,y_train_split, batch_size = 4, epochs=25)

  results = model.evaluate(x_valid_split, y_valid_split, batch_size=4)
  pred = model.predict(x_valid_split)
  pred = np.argmax(pred,axis=-1)
  true_labels_split = np.argmax(y_valid_split,axis=-1)
  results = results[0:2] + list(precision_recall_fscore_support(true_labels_split, pred, average = 'macro')[:-1])
  results = [round(x,4) for x in results]
  validation_errors.append(results)
  # need to save the best model to then use in test
  if len(validation_errors) == 1:
    best_loss = validation_errors[0][0]
    model.save('/content/drive/MyDrive/Datawithoutanything/VGG')

  elif best_loss > validation_errors[-1][0]:
    model.save('/content/drive/MyDrive/Datawithoutanything/VGG')
    best_loss = validation_errors[-1][0]


# Evaluate the model on the test data using `evaluate`
print("Evaluate on test data")
model = tf.keras.models.load_model('/content/drive/MyDrive/Datawithoutanything/VGG')
model.compile(optimizer = 'adam', loss = 'categorical_crossentropy',
                metrics=[keras.metrics.CategoricalAccuracy()])
test_results = model.evaluate(x_test, y_test, batch_size=4)
pred = model.predict(x_test)
pred = np.argmax(pred,axis=-1)
true_labels = np.argmax(y_test,axis=-1)
test_results = test_results[0:2] + list(precision_recall_fscore_support(true_labels, pred, average = 'macro')[:-1])
test_results = [round(x,4) for x in test_results]
print("test loss, test acc:", test_results)

# Evaluate the model on the test data using `evaluate`
print("Evaluate on test data")
model = tf.keras.models.load_model('/content/drive/MyDrive/Datawithoutanything/VGG')
model.compile(optimizer = 'adam', loss = 'categorical_crossentropy',
                metrics=[keras.metrics.CategoricalAccuracy()])
test_results = model.evaluate(images_raw,keras.utils.to_categorical(labels ,4), batch_size=4)
pred = model.predict(images_raw)
pred = np.argmax(pred,axis=-1)
true_labels = np.argmax(keras.utils.to_categorical(labels ,4),axis=-1)
test_results = test_results[0:2] + list(precision_recall_fscore_support(true_labels, pred, average = 'macro')[:-1])
test_results = [round(x,4) for x in test_results]

#MNIST VGG16
VGG_16_model = tf.keras.applications.VGG16(
    include_top=False, weights='imagenet', )

def build_VGG(pretrained):
  
  inputs = keras.Input(shape = (48,48,3))

  x = layers.Conv2D(64,(3,3),padding='same', activation = 'relu')(inputs)
  x = layers.Conv2D(64,(3,3),padding='same', activation = 'relu')(x)
  x = layers.MaxPooling2D(pool_size=(2,2), strides = (2,2))(x)

  x = layers.Conv2D(128, (3,3), padding = 'same', activation ='relu')(x)
  x = layers.Conv2D(128, (3,3), padding = 'same', activation ='relu')(x)
  x = layers.MaxPooling2D(pool_size=(2,2), strides = (2,2))(x)

  x = layers.Conv2D(256, (3,3), padding = 'same', activation ='relu')(x)
  x = layers.Conv2D(256, (3,3), padding = 'same', activation ='relu')(x)
  x = layers.Conv2D(256, (3,3), padding = 'same', activation ='relu')(x)
  x = layers.MaxPooling2D(pool_size=(2,2), strides = (2,2))(x)

  x = layers.Conv2D(512, (3,3), padding = 'same', activation ='relu')(x)
  x = layers.Conv2D(512, (3,3), padding = 'same', activation ='relu')(x)
  x = layers.Conv2D(512, (3,3), padding = 'same', activation ='relu')(x)
  x = layers.MaxPooling2D(pool_size=(2,2), strides = (2,2))(x)

  x = layers.Conv2D(512, (3,3), padding = 'same', activation ='relu')(x)
  x = layers.Conv2D(512, (3,3), padding = 'same', activation ='relu')(x)
  x = layers.Conv2D(512, (3,3), padding = 'same', activation ='relu')(x)
  x = layers.MaxPooling2D(pool_size=(2,2), strides = (2,2))(x)

  outputs = layers.Flatten()(x)
  model = keras.Model(inputs, outputs, name = 'VGG16')

  model.compile(optimizer = 'adam', loss = 'categorical_crossentropy')
  
  #transfer learning
  model.set_weights(pretrained.get_weights())
  assert len(model.weights) == len(pretrained.weights)
  for a, b in zip(model.weights, pretrained.weights):
    np.testing.assert_allclose(a.numpy(), b.numpy())



  x = layers.Dense(512, activation = 'relu')(model.layers[-1].output)
  x = layers.Dense(10, activation = 'softmax')(x)

  model = keras.Model(inputs = inputs, outputs =x, name = 'VGG16')



  model.compile(optimizer = keras.optimizers.Adam(), loss = 'categorical_crossentropy',
                metrics=[keras.metrics.CategoricalAccuracy()])
  
  return model

(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()

x_train = x_train.reshape(-1, 28, 28) / 255
x_train = np.stack([x_train,x_train,x_train], axis=-1)
x_test = x_test.reshape(-1, 28, 28) / 255
x_test = np.stack([x_test,x_test,x_test], axis=-1)


from keras.preprocessing.image import img_to_array, array_to_img
x_train = np.asarray([img_to_array(array_to_img(im, scale=False).resize((48,48))) for im in x_train])
x_test = np.asarray([img_to_array(array_to_img(im, scale=False).resize((48,48))) for im in x_test])


y_train = keras.utils.to_categorical(y_train, 10)
y_test = keras.utils.to_categorical(y_test, 10)

def change_shape(x):
  # Change the shape to (48, 48, 3)
  x = np.reshape(x, (len(x), 28, 28, 1))
  # Current shape (len, 28, 28, 1)
  x = tf.image.grayscale_to_rgb(tf.convert_to_tensor(x))
  # Current shape (len, 28, 28, 3)
  x = np.array(tf.image.resize(x, [48, 48]))
  # Current shape (48, 48, 3)
  # Normalise the data and change data type
  x = x / 255.
  x = x.astype('float32')
  # Preprocess input
  return x
(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()
x_train = change_shape(x_train)
x_test = change_shape(x_test)
#one hot vector encoding
y_train = keras.utils.to_categorical(y_train, 10)
y_test = keras.utils.to_categorical(y_test, 10)

model = build_VGG(VGG_16_model)
history = model.fit(x_train,y_train, batch_size = 256, shuffle = True,epochs=15,validation_split=0.1,
                    callbacks = tf.keras.callbacks.ReduceLROnPlateau(
                              monitor='val_loss',
                              factor=0.2,
                              patience=2,
                              verbose=1,
                              mode='auto',
                              epsilon=0.0001,
                              cooldown=2,
                              min_lr=0))

results,accuracy = model.evaluate(x_test, y_test, batch_size=256)
pred = model.predict(x_test)
pred = np.argmax(pred,axis=-1)

